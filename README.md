# Pipeline de Donn√©es Agricoles

Ce projet fournit un pipeline robuste de data engineering pour le traitement de donn√©es agricoles √† l‚Äôaide d‚Äô**Azure Data Factory**, **Azure Databricks** et **Azure Synapse Analytics**.  
Le pipeline automatise l‚Äôingestion, le traitement et le stockage des donn√©es dans les couches **bronze**, **silver** et **gold**, pr√©parant les donn√©es pour l‚Äôanalyse et la visualisation.

---

## Cas d‚ÄôUsage

Les donn√©es agricoles (m√©t√©o, sol, production) sont essentielles pour am√©liorer les rendements, anticiper les conditions climatiques et g√©rer les ressources de mani√®re durable.  
Les parties prenantes telles que les agriculteurs, les ing√©nieurs agronomes, les instituts de recherche ou les d√©cideurs politiques s‚Äôappuient sur des informations pr√©cises et √† jour pour :

- Planifier les cultures et les ressources.
- √âvaluer les risques li√©s au climat.
- Prendre des d√©cisions √©clair√©es bas√©es sur les donn√©es.

En automatisant la collecte et la transformation des donn√©es agricoles, ce pipeline permet aux utilisateurs de **pr√©dire, analyser et optimiser** les activit√©s agricoles avec pr√©cision et rapidit√©.

## Architecture & Technologies

Ce projet repose sur une architecture moderne de type **Medallion** (Bronze / Silver / Gold) pour le traitement et l‚Äôanalyse des donn√©es m√©t√©orologiques.

### Pipeline global

Le pipeline suit cette structure logique :

1. **Azure Data Factory (ADF)** : Orchestration de l‚Äôingestion depuis un fichier CSV de villes et ex√©cution de notebooks.
2. **Azure Data Lake Storage Gen2** : Stockage des donn√©es brutes, semi-trait√©es et enrichies.
3. **Azure Databricks** : Traitement Spark distribu√© en 3 couches :
   - **Bronze** : Donn√©es brutes collect√©es via une API m√©t√©o.
   - **Silver** : Donn√©es nettoy√©es, structur√©es (s√©paration date/heure, format parquet).
   - **Gold** : Donn√©es enrichies (ville, pays, classification de temp√©rature).
4. **Azure Synapse Analytics** : Lecture et requ√™tage des donn√©es Parquet depuis le lac.
5. **Power BI** : Cr√©ation de visualisations interactives √† partir des donn√©es analys√©es.

### Architecture visuelle

![Architecture Pipeline](./agri-pipline.png)

### Stack technique utilis√©e

| Composant             | R√¥le principal                          |
|-----------------------|------------------------------------------|
| Azure Data Factory    | Orchestration des notebooks              |
| Azure Storage Gen2    | Stockage structur√© en Bronze/Silver/Gold |
| Azure Databricks      | Traitement Spark, API, enrichissement    |
| Azure Synapse         | Requ√™tes SQL sur Parquet                 |
| Power BI              | Visualisation des donn√©es finales        |
| API OpenMeteo         | Source de donn√©es m√©t√©o                  |
| Python + PySpark      | Traitement de donn√©es                    |
| Librairie `reverse_geocoder` | R√©cup√©ration ville/pays √† partir des coordonn√©es |

---


## Azure Setup

Cette section d√©crit la mise en place des services Azure n√©cessaires au fonctionnement du pipeline. L‚Äôobjectif est d‚Äôassurer une communication fluide entre les diff√©rentes briques du projet.

### 1. Cr√©ation des ressources Azure

Plusieurs services Azure ont √©t√© provisionn√©s pour mettre en ≈ìuvre l‚Äôarchitecture Medallion :

- **Azure Data Lake Storage Gen2**  
  Utilis√© pour stocker les fichiers dans les diff√©rentes couches : `bronze`, `silver` et `gold`.  
  ‚Üí Trois conteneurs ont √©t√© cr√©√©s : `bronze`, `silver` et `gold`.

- **Azure Databricks**  
  Un workspace Databricks a √©t√© cr√©√© pour ex√©cuter les traitements distribu√©s avec PySpark.  
  Un cluster a √©t√© lanc√©, avec la librairie `reverse_geocoder` install√©e pour enrichir les donn√©es par g√©olocalisation.

- **Azure Synapse Analytics**  
  Cr√©√© pour interroger les fichiers Parquet pr√©sents dans la couche Gold, via le langage SQL.

- **Azure Data Factory (ADF)**  
  Utilis√© pour orchestrer l‚Äôensemble du pipeline : d√©clenchement automatique chaque jour √† minuit, it√©ration sur les villes, ex√©cution des notebooks.

---

### 2. Connexion entre services

Afin que tous les services puissent interagir avec le Data Lake, plusieurs √©tapes de configuration ont √©t√© n√©cessaires :

- **Cr√©ation des credentials + external location dans Databricks**  
  Ces √©l√©ments permettent √† Databricks d‚Äôacc√©der aux fichiers dans le Data Lake via les chemins `abfss://`.

- **Gestion des r√¥les et permissions (IAM)**  
  Il a fallu attribuer le r√¥le suivant au service principal de Databricks :  
  ‚Üí `Storage Blob Data Contributor`  
  Ceci a √©t√© fait au niveau du **container** et non uniquement au niveau du compte de stockage.
!(./images/credentiel.png)
!(./images/IAM control.png)

---
## Databricks ‚Äì Traitement des donn√©es

Le traitement des donn√©es se fait en trois √©tapes √† l‚Äôaide de notebooks PySpark dans Databricks. Chaque notebook correspond √† une couche du pipeline : Bronze, Silver ou Gold.

---

### 1. Configuration du cluster et librairie

Un cluster Databricks a √©t√© cr√©√© avec un runtime compatible PySpark.  
Nous avons install√© la librairie suivante dans le cluster :

- `reverse_geocoder` ‚Üí permet d'enrichir chaque ligne avec le nom de la ville et le code pays √† partir de la latitude et longitude.

---

### 2. Bronze Notebook ‚Äì R√©cup√©ration des donn√©es m√©t√©o

#### √âtapes du traitement :

- Lecture des param√®tres transmis par Azure Data Factory : `latitude`, `longitude`, `ville`, `pays`, `today`
- Requ√™te HTTP vers l‚ÄôAPI [Open-Meteo](https://open-meteo.com/), avec r√©cup√©ration des param√®tres :
  - `temperature_2m`
  - `soil_temperature_0cm`
  - `precipitation`
- V√©rification du contenu retourn√© par l‚ÄôAPI
- Ajout des champs `latitude`, `longitude`, `ville`, `pays`, `date` dans le JSON
- Sauvegarde d‚Äôun fichier JSON par ville et par jour dans le conteneur **bronze**, chemin :
<pre>abfss://bronze@agristorage2025.dfs.core.windows.net/meteo/<pays><ville><today>.json</pre>

---

### 3. Silver Notebook ‚Äì Transformation des donn√©es

Le notebook Silver lit les fichiers JSON du Bronze, nettoie et structure les donn√©es.

#### Traitements effectu√©s :

- Extraction des listes `time`, `temperature_2m`, `soil_temperature_0cm`, `precipitation`
- Utilisation de `posexplode` pour aligner chaque valeur sur son horodatage
- Conversion du champ `time` en type `timestamp`
- S√©paration de `datetime` en deux colonnes : `date` (YYYY-MM-DD) et `heure` (HH:mm)
- Ajout des colonnes `latitude`, `longitude`, `ville`, `pays`
- Enregistrement du r√©sultat au format **Parquet** dans le conteneur Silver :

<pre> abfss://silver@agristorage2025.dfs.core.windows.net/meteo/<pays><ville><today>.parquet</pre>

---

### 4. Gold Notebook ‚Äì Enrichissement et pr√©paration finale

Le Gold Notebook lit tous les fichiers Silver du jour, enrichit les donn√©es et les agr√®ge.

#### √âtapes cl√©s :

- Lecture de tous les fichiers du jour :

<pre> df = spark.read.parquet(f"{silver_adls}/meteo/*_{today}.parquet") </pre>

- Ajout des colonnes country_code et city √† l‚Äôaide de la fonction reverse_geocoder

- Un UDF PySpark personnalis√© a √©t√© utilis√© pour int√©grer reverse_geocoder dans le pipeline Spark.

- Ajout d'une colonne stemp_class qui cat√©gorise les temp√©ratures :
   - soil_temperature_0cm <0¬∞C ‚Üí gel
   - soil_temperature_0cm 0¬∞C et 10¬∞C ‚Üí froid
   - soil_temperature_0cm entre 10¬∞C et 20¬∞C ‚Üí mod√©r√©
   - soil_temperature_0cm entre 20¬∞C et 30¬∞C ‚Üí chaud
   - soil_temperature_0cm > 30¬∞C ‚Üí tr√®s chaud
   - √âcriture dans le conteneur Gold :
  <pre>abfss://gold@agristorage2025.dfs.core.windows.net/weather_gold/<today>/</pre>
#### Sch√©ma final :
- `date`
- `heure`
- `temperature_2m`
- `soil_temperature_0cm`
- `precipitation`
- `latitude`,  `longitude`
- `country_code`,  `city`
- `stemp_class`
---
## Azure Data Factory ‚Äì Orchestration du pipeline

L'orchestration compl√®te du pipeline a √©t√© mise en place avec **Azure Data Factory (ADF)**, en utilisant des blocs visuels comme `Lookup`, `ForEach`, et `Execute Notebook`. L'objectif est d'automatiser le traitement des donn√©es m√©t√©o pour plusieurs villes chaque jour.

---

### 1. Ingestion multi-villes

La premi√®re √©tape consiste √† importer un **fichier CSV** contenant la liste des villes √† traiter. Ce fichier contient les colonnes suivantes :

- `ville`
- `pays`
- `latitude`
- `longitude`

Ce fichier est stock√© dans un **dataset ADF** de type CSV connect√© √† un blob storage ou Azure Data Lake.

#### √âtapes dans le pipeline :
- Un bloc **Lookup** est utilis√© pour lire le fichier.
- Ce lookup renvoie une liste d‚Äôobjets JSON (une par ville).
- Un bloc **ForEach** permet d‚Äôit√©rer sur chaque ligne.

Dans la boucle `ForEach`, plusieurs param√®tres sont pass√©s √† Databricks pour chaque ex√©cution :

- `ville`
- `pays`
- `latitude`
- `longitude`
- `today` (g√©n√©r√© dynamiquement dans ADF avec `@utcNow()`)

---

### 2. Orchestration compl√®te

√Ä l‚Äôint√©rieur du bloc `ForEach`, deux notebooks sont appel√©s de mani√®re s√©quentielle :

#### a. Bronze Notebook
- R√©cup√®re les donn√©es depuis l‚ÄôAPI Open-Meteo
- Stocke les fichiers JSON dans Bronze

#### b. Silver Notebook
- Lit les fichiers Bronze
- Transforme et stocke les donn√©es en Parquet dans Silver

> Les deux notebooks utilisent `dbutils.widgets.get(...)` pour r√©cup√©rer les param√®tres pass√©s depuis ADF.

#### c. Gestion des retours de notebook
Chaque notebook retourne un dictionnaire JSON s√©rialis√© avec `dbutils.notebook.exit(json.dumps(...))`.  
Ce retour permet de transmettre des informations √† l‚Äô√©tape suivante, comme `today`, `silver_adls`, `gold_adls`, etc.

---

### 3. Ex√©cution finale ‚Äì Gold Notebook

Une fois la boucle `ForEach` termin√©e, un troisi√®me notebook est ex√©cut√© **hors de la boucle** :

- Il r√©cup√®re tous les fichiers Silver du jour (`*_{today}.parquet`)
- Applique l'enrichissement (ville/pays + classification)
- Stocke les donn√©es finales dans la couche Gold

Les param√®tres (`today`, `silver_adls`, `gold_adls`) sont transmis √† ce notebook soit via le premier `Bronze Notebook`, soit fix√©s dans ADF.

---

### 4. D√©clencheur automatique (Trigger)

Pour automatiser l'ex√©cution, un **Schedule Trigger** a √©t√© configur√© dans ADF :

- **Fr√©quence** : quotidienne
- **Heure** : 00:00 (minuit UTC)
- **Action** : d√©clenchement complet du pipeline

Cela permet d‚Äôavoir une **mise √† jour automatique des donn√©es m√©t√©o** sans intervention manuelle.

---

## üîç Synapse & Power BI

Une fois les donn√©es enrichies et stock√©es dans la couche Gold (au format Parquet), elles peuvent √™tre analys√©es directement via **Azure Synapse Analytics** en mode serverless, puis visualis√©es dans **Power BI**.

---

### 1. Lecture des fichiers Parquet avec OPENROWSET

Dans Synapse, il n‚Äôest pas n√©cessaire d'importer les donn√©es. On peut interroger directement les fichiers Parquet stock√©s dans le Data Lake gr√¢ce √† la fonction `OPENROWSET`.

#### Exemple de requ√™te :

<pre>
SELECT *
FROM OPENROWSET(
    BULK 'https://agristorage2025.dfs.core.windows.net/gold/weather_gold/**',
    FORMAT = 'PARQUET'
) AS meteo  </pre>

### 2. Connexion √† Synapse depuis Power BI
Dans Power BI Desktop :

- Accueil > Obtenir les donn√©es

- On choisis Azure Synapse Analytics (SQL Serverless)

- Server : Serverless SQL endpoint
- Base de donn√©es : master et on √©crirs une requ√™te manuelle

## Visualisations Power BI

Le tableau de bord Power BI permet d‚Äôexplorer facilement les donn√©es m√©t√©o r√©cup√©r√©es par le pipeline, √† l‚Äô√©chelle mondiale.

### Visuels utilis√©s

| Type de visuel            | Donn√©es utilis√©es                                     |
|---------------------------|--------------------------------------------------------|
| **Courbe**                | Temp√©rature (`temperature_2m`) par heure (`heure`)    |
| **Carte g√©ographique**    | Coordonn√©es (`latitude`, `longitude`)                 |
| **Histogramme / Camembert** | Nombre d'observations par `stemp_class` (mod√©r√©, chaud‚Ä¶) |
| **Segments (filtres)**    | Champs : `ville`, `pays`, `date`, `stemp_class`       |

### Extrait du dashboard :

![Dashboard m√©t√©o Power BI](./images/dashboard_ville_global.png)

Ce dashboard permet de :
- Comparer l‚Äô√©volution horaire de la temp√©rature par ville
- Visualiser la r√©partition g√©ographique des temp√©ratures moyennes
- Explorer les classes de temp√©rature par pays
- Appliquer des filtres dynamiques pour zoomer sur une ville ou une classe







